{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(corpus,term_dic): #Create function (arg1,arg2....)\n",
    "    # treat every paragraph as a document : N=num_of_para\n",
    "    num_of_para=len(corpus)\n",
    "    temp_dic={}\n",
    "    for term in term_dic:\n",
    "        doc_freq=0\n",
    "        term_freq=0\n",
    "        for paragraph in corpus:\n",
    "            if (paragraph.find(term)>=0):\n",
    "                doc_freq+=1\n",
    "                term_freq+=paragraph.count(term)     \n",
    "        if doc_freq==0:\n",
    "            weight=0\n",
    "        else:\n",
    "            weight=(1 + math.log10(term_freq)) * math.log10(num_of_para/doc_freq)\n",
    "        temp_dic[term] = weight\n",
    "    return temp_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from six import iteritems\n",
    "import xgboost as xgb\n",
    "from math import log\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "with open('train.txt', encoding='utf-8') as f:\n",
    "    train_data = f.readlines()\n",
    "    f.close\n",
    "with open('test.txt', encoding='utf-8') as f:\n",
    "    test_data = f.readlines()\n",
    "    f.close\n",
    "with open('Dream_of_the_Red_Chamber_seg.txt', encoding='utf-8') as f:\n",
    "    corpus_seg = f.readlines()\n",
    "    f.close\n",
    "with open('Dream_of_the_Red_Chamber.txt', encoding='utf-8') as f:\n",
    "    corpus = f.readlines()\n",
    "    f.close\n",
    "\n",
    "# Test & Train Array Converter\n",
    "TRAIN = []\n",
    "index = 0\n",
    "for row in train_data:\n",
    "    if index == 0:\n",
    "        index += 1\n",
    "        continue\n",
    "    index += 1\n",
    "    x = re.split('\\t|\\n', row)\n",
    "    TRAIN.append([x[1], x[2], x[3]])\n",
    "    \n",
    "TEST = []\n",
    "# ========= For Learning Base Global Constant ==================\n",
    "index = 0\n",
    "for row in test_data:\n",
    "    if index == 0:\n",
    "        index += 1\n",
    "        continue\n",
    "    index += 1\n",
    "    x = re.split('\\t|\\n', row)\n",
    "    TEST.append([x[1], x[2], x[3]])\n",
    "\n",
    "# Build The Word Vector\n",
    "TERM_LIST = []\n",
    "for paragraph in corpus_seg:\n",
    "    tokens = paragraph.split()\n",
    "    for token in tokens:\n",
    "        if '_P' in token:\n",
    "            continue\n",
    "        #term = re.sub('_[A-Z | a-z | 0-9]*', '', token)\n",
    "        term = token.split('_')[0]\n",
    "        if term not in TERM_LIST:\n",
    "            TERM_LIST.append(term)\n",
    "\n",
    "RELATION = {\n",
    "    '祖孫': 0,\n",
    "    '母子': 1,\n",
    "    '母女': 2,\n",
    "    '父子': 3,\n",
    "    '父女': 4,\n",
    "    '兄弟姊妹': 5,\n",
    "    '夫妻': 6,\n",
    "    '姑叔舅姨甥侄': 7,\n",
    "    '遠親': 8,\n",
    "    '主僕': 9,\n",
    "    '師徒': 10,\n",
    "    '居處': 11,\n",
    "}\n",
    "\n",
    "PERSON = {}\n",
    "index = 1\n",
    "for x in TRAIN:\n",
    "    per1 = x[0]\n",
    "    per2 = x[1]\n",
    "    if per1 not in PERSON:\n",
    "        PERSON[per1] = index\n",
    "        index += 1\n",
    "    if per2 not in PERSON:\n",
    "        PERSON[per2] = index\n",
    "        index += 1\n",
    "for x in TEST:\n",
    "    per1 = x[0]\n",
    "    per2 = x[1]\n",
    "    if per1 not in PERSON:\n",
    "        PERSON[per1] = index\n",
    "        index += 1\n",
    "    if per2 not in PERSON:\n",
    "        PERSON[per2] = index\n",
    "        index += 1\n",
    "        \n",
    "GENERAL_NAME = [\n",
    "    '婆子', '夫人', '大姐', '小姐', '嫂子', '姨娘',  '姨媽', '嬸娘', '嫂子', '老娘', '嬤嬤', '奶奶'\n",
    "]\n",
    "\n",
    "Group1 = ['嫁','娶','婚','買','嫡夫','婦','嫡','妻','妾','連理','太太','夫妻']\n",
    "Group2 = ['喚作','取名','生','有了','得了','養','懷','爹','娘','父','母','女','女兒','子','孩','乳名','小名']\n",
    "Group3 = ['請','給','來','請安','磕頭','問好','跪','稟明','奉','喚來','叫','祖','奶','孫','老太太','帶','領']\n",
    "Group4 = ['長','次','大']\n",
    "Group5 = ['兄','哥','弟','姊','姐','妹']\n",
    "Group6 = ['姑','叔','舅','姨','甥','侄','親']\n",
    "Group7 = ['帶','領','教','徒','門生','師父']\n",
    "Group8 = ['主','僕','丫','丫頭','丫鬟','心腹','小的','下人','主僕']\n",
    "Group9 = ['使喚','謝','領','接','扇','差','命','遣','迎','打發','吩咐','喚','罵']\n",
    "\n",
    "GROUPS = {\n",
    "    '婚配': Group1, \n",
    "    '直系': Group2, \n",
    "    '尊卑': Group3, \n",
    "    '旁系': Group4, \n",
    "    '手足': Group5, \n",
    "    '遠親': Group6, \n",
    "    '師徒': Group7, \n",
    "    '主僕': Group8,\n",
    "    '命令': Group9\n",
    "}\n",
    "\n",
    "CORPUS = corpus\n",
    "\n",
    "PRIORITY = [8, 4, 2, 1]\n",
    "# ==============For Rule Base Global Constant=============\n",
    "# Build The Dictinary\n",
    "term_dic = {} # {term:代號,...}\n",
    "term_weight_dic = {}\n",
    "featureDic={}\n",
    "\n",
    "                \n",
    "# for row in TRAIN:\n",
    "#     if row[0] not in term_dic:\n",
    "#         term_dic[row[0]] = 'Character'\n",
    "#     if row[1] not in term_dic:\n",
    "#         term_dic[row[1]] = 'Character'\n",
    "        \n",
    "# for row in TEST:\n",
    "#     if row[0] not in term_dic:\n",
    "#         term_dic[row[0]] = 'Character'\n",
    "#     if row[1] not in term_dic:\n",
    "#         term_dic[row[1]] = 'Character'\n",
    "    \n",
    "for paragraph in corpus_seg:\n",
    "    tokens = paragraph.split()\n",
    "    for token in tokens:\n",
    "        # Normal Norm\n",
    "        if '_Na' in token:\n",
    "            pair = token.split('_')\n",
    "            if pair[0] not in term_dic:\n",
    "                term_dic[pair[0]]=pair[1]\n",
    "        if '_Nb' in token:\n",
    "            pair = token.split('_')\n",
    "            if pair[0] not in term_dic:\n",
    "                term_dic[pair[0]]=pair[1]\n",
    "        # Location\n",
    "        elif '_Nc' in token:\n",
    "            pair = token.split('_')\n",
    "            if pair[0] not in term_dic:\n",
    "                term_dic[pair[0]]=pair[1]\n",
    "        # Time\n",
    "        elif '_Nd' in token:\n",
    "            pair = token.split('_')\n",
    "            if pair[0] not in term_dic:\n",
    "                term_dic[pair[0]]=pair[1]\n",
    "        elif '_V' in token:\n",
    "            pair = token.split('_')\n",
    "            if pair[0] not in term_dic:\n",
    "                term_dic[pair[0]]=pair[1]                \n",
    "\n",
    "                \n",
    "term_weight_dic = tf_idf(corpus,term_dic)\n",
    "\n",
    "featureDic['婚配']=['嫁','娶','婚','嫡夫','婦','嫡','妻','妾','連理','太太','夫妻', '媳婦', '夫婦']\n",
    "featureDic['直系']=['喚作','取名','生','有了','得了','養','懷','爹','娘','父','母','兒','女','女兒','子','孩','乳名','小名']\n",
    "featureDic['尊卑']=['請','給','來','請安','磕頭','問好','跪','稟明','奉','喚來','叫','祖','奶','孫','老太太','帶','領']\n",
    "featureDic['旁系']=['長','次','大']\n",
    "featureDic['手足']=['兄','哥','弟','姊','姐','妹']\n",
    "featureDic['遠親']=['姑','叔','舅','姨','甥','侄','親']\n",
    "featureDic['師徒']=['帶','領','教','徒','門生','師父']\n",
    "featureDic['主僕']=['主','僕','丫','丫頭','丫鬟','心腹','小的','下人','主僕']\n",
    "featureDic['命令']=['使喚','謝','領','接','扇','差','命','遣','迎','打發','吩咐','喚','罵']\n",
    "featureDic['女性']=['嬤','母','姐','姊','妹','太','夫人','氏','娘','女','姑','姨']\n",
    "featureDic['夫姓']=['姐','母','娘','媽','奶','嬤']    # 若冠夫姓或父姓，可能會出現的稱呼\n",
    "featureDic['地點']=[]\n",
    "for key in term_dic:\n",
    "    if 'Nc' in term_dic[key]:\n",
    "        featureDic['地點'].append(key)\n",
    "\n",
    "relationDic={'祖孫':0, '母子':1, '母女':2, '父子':3, '父女':4, '兄弟姊妹':5,'夫妻':6,\n",
    "             '姑叔舅姨甥侄':7,'遠親':8,'主僕':9, '師徒':10,'居處':11 }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isGeneralName(name):\n",
    "    if len(name) == 2:\n",
    "        return False\n",
    "    else:\n",
    "        subname = name[1:]\n",
    "        if subname in GENERAL_NAME:\n",
    "            return True\n",
    "\n",
    "def isContainName(content, general_tag, name):\n",
    "    if name in content:\n",
    "        return True\n",
    "    elif(general_tag):\n",
    "        first_name = name[0]\n",
    "        last_name = name[1:]\n",
    "        if first_name in content and last_name in content:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "class FeatureExtractor:\n",
    "    \n",
    "    def __init__(self, per1, per2, rel, file, feature_type):\n",
    "        self.per1 = per1\n",
    "        self.per2 = per2\n",
    "        self.rel = rel\n",
    "        self.file = file\n",
    "        self.feature_type = feature_type\n",
    "        self.word_vector = {}\n",
    "        self.features = {}\n",
    "        self.initialize()\n",
    "    def initialize(self):\n",
    "        \n",
    "        # FEATURE0 - RELATION\n",
    "        self.features['關係'] = 12\n",
    "        \n",
    "        # Intialized the word vector\n",
    "        for term in TERM_LIST:\n",
    "            self.word_vector[term] = 0\n",
    "            \n",
    "        # Added Person Number(e.g. 曹雪芹: 1, 賈寶玉: 2...)\n",
    "        p1 =  PERSON[self.per2] if PERSON[self.per1] > PERSON[self.per2] else PERSON[self.per1]\n",
    "        p2 =  PERSON[self.per1] if PERSON[self.per1] > PERSON[self.per2] else PERSON[self.per2]\n",
    "        \n",
    "        # FEATURE - Per_1\n",
    "        # FEATURE - Per_2\n",
    "        self.features['角色一'] = p1\n",
    "        self.features['角色二'] = p2\n",
    "            \n",
    "        #FEATURE - Term_Group\n",
    "        for feature, value in iteritems(GROUPS):\n",
    "            self.features[feature] = 0\n",
    "            \n",
    "        # FEATURE - Last_name\n",
    "        # Determine the last name is same or not\n",
    "        if self.per1[0] == self.per2[0]:\n",
    "            self.features['姓'] = 100\n",
    "        else:\n",
    "            self.features['姓'] = 0\n",
    "    def extract(self, content, priority):\n",
    "        weight = priority\n",
    "        tokens = content.split()\n",
    "\n",
    "        for token in tokens:\n",
    "            \n",
    "            # Useless Term\n",
    "            if '_P' in token:\n",
    "                continue\n",
    "            elif '_DE' in token:\n",
    "                continue\n",
    "            elif '_T' in token:\n",
    "                continue\n",
    "            elif '_SHI' in token:\n",
    "                continue\n",
    "            \n",
    "            term = token.split('_')[0]\n",
    "            for feature, rule in iteritems(GROUPS):\n",
    "                weight = priority\n",
    "                if term in rule:\n",
    "                    self.features[feature] += 1 * weight                \n",
    "            self.word_vector[term] += 1 * priority\n",
    "    def save(self):\n",
    "        word_freq = ''\n",
    "        feature_str = ''\n",
    "        \n",
    "        for word, freq in iteritems(self.word_vector):\n",
    "            word_freq = word_freq + str(freq) + ','\n",
    "            \n",
    "        for feature, score in iteritems(self.features):\n",
    "            feature_str = feature_str + str(score) + ','\n",
    "        \n",
    "        if (self.feature_type == 'TERMnGROUP'):\n",
    "            self.file.write(str(RELATION[self.rel]) + ',' + word_freq + \",\" + feature_str[:-1] + '\\n')\n",
    "        elif (self.feature_type == 'GROUP'):\n",
    "            self.file.write(str(RELATION[self.rel]) + ',' + feature_str[:-1] + '\\n')\n",
    "        elif (self.feature_type == 'TERM'):\n",
    "            self.file.write(str(RELATION[self.rel]) + ',' + word_freq[:-1] + '\\n')\n",
    "        else:\n",
    "            raise ValueError('Wrong Feature Type!')\n",
    "        \n",
    "        \n",
    "class Preprocessor:\n",
    "    \n",
    "    def __init__(self, data, corpus, filename):\n",
    "        self.file = open(filename, 'w');\n",
    "        self.data = data\n",
    "        self.extracted = [None] * len(data)\n",
    "        self.corpus = corpus\n",
    "        self.vector = {}\n",
    "        \n",
    "    def close(self):\n",
    "        self.file.close();\n",
    "        \n",
    "    def transform(self, feature_type):\n",
    "        index = 0\n",
    "        for row in self.data:\n",
    "            extracted = []\n",
    "            extractor = FeatureExtractor(row[0], row[1], row[2], self.file, feature_type)\n",
    "            \n",
    "            per1 = row[0]\n",
    "            per2 = row[1]\n",
    "            per1_general = isGeneralName(per1)\n",
    "            per2_general = isGeneralName(per2)\n",
    "            \n",
    "            tag = False\n",
    "            \n",
    "            # Sentence\n",
    "            for paragraph in self.corpus:\n",
    "                sentences = re.split('，|。|？|！|；', paragraph)\n",
    "                for i in range(len(sentences)):\n",
    "                    if isContainName(sentences[i], per1_general, per1) and isContainName(sentences[i], per2_general, per2):\n",
    "                        extractor.extract(sentences[i], PRIORITY[0])\n",
    "                        extracted.append('S: ' + sentences[i])\n",
    "                        if (tag == False):\n",
    "                            tag = True\n",
    "            \n",
    "            # Context\n",
    "            if tag == False:\n",
    "                for paragraph in self.corpus:\n",
    "                    sentences = re.split('，|。|？|！|；', paragraph)\n",
    "                    for i in range(len(sentences)-2):\n",
    "                        context = sentences[i] + sentences[i+1] + sentences[i+2]\n",
    "                        if isContainName(context, per1_general, per1) and isContainName(context, per2_general, per2):\n",
    "                            extractor.extract(context, PRIORITY[1])\n",
    "                            extracted.append('C: ' + context)\n",
    "                            if (tag == False):\n",
    "                                tag = True\n",
    "            # Otherwise\n",
    "            if tag == False:\n",
    "                temp = ['', '']\n",
    "                for paragraph in self.corpus:\n",
    "                    sentences = re.split('，|。|？|！|；', paragraph)\n",
    "                    for sentence in sentences:\n",
    "                        if isContainName(sentence, per1_general, per1) and temp[0] == '':\n",
    "                            temp[0] = sentence\n",
    "                        if isContainName(sentence, per2_general, per2) and temp[1] == '':\n",
    "                            temp[1] = sentence\n",
    "                    if temp[0] != '' and temp[1] != '':\n",
    "                        extractor.extract(temp[0] + ' ' + temp[1], PRIORITY[3])\n",
    "                        extracted.append('O: ' + temp[0] + ' ' + temp[1])\n",
    "                        if tag == False:\n",
    "                            tag = True\n",
    "                        break;\n",
    "                        \n",
    "            # Otherwise\n",
    "            if tag == False:\n",
    "                temp = ['', '']\n",
    "                for paragraph in CORPUS:\n",
    "                    sentences = re.split('，|。|？|！|；', paragraph)\n",
    "                    for sentence in sentences:\n",
    "                        if per1 in sentence and temp[0] == '':\n",
    "                            temp[0] = sentence\n",
    "                        if per2 in sentence and temp[1] == '':\n",
    "                            temp[1] = sentence\n",
    "                    if temp[0] != '' and temp[1] != '':\n",
    "                        extracted_content = temp[0] + ' ' + temp[1]\n",
    "                        extracted.append('R: ' + extracted_content)\n",
    "                        break;\n",
    "                \n",
    "            self.extracted[index] = extracted\n",
    "            index +=1\n",
    "            extractor.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFeatureFile(train, test, corpus, feature_type):\n",
    "    # Training Data & Testing Data Transformation\n",
    "    pre1 = Preprocessor(train, corpus, 'ftrain.txt')\n",
    "    pre1.transform(feature_type)\n",
    "    pre1.close()\n",
    "\n",
    "    pre2 = Preprocessor(test, corpus, 'ftest.txt')\n",
    "    pre2.transform(feature_type)\n",
    "    pre2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def xgboostTraining(depth, objective):\n",
    "    dtrain = xgb.DMatrix('ftrain.txt')\n",
    "    dtest = xgb.DMatrix('ftest.txt')\n",
    "    # specify parameters via map\n",
    "    param = {}\n",
    "    # use softmax multi-class classification\n",
    "    param['objective'] = 'multi:soft' + objective\n",
    "    # scale weight of positive examples\n",
    "    param['eta'] = 0.1126\n",
    "    param['max_depth'] = depth\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 12\n",
    "    num_round = 2\n",
    "    bst = xgb.train(param, dtrain, num_round)\n",
    "    # make prediction\n",
    "    bst.save_model('temp.txt')\n",
    "    bst = xgb.Booster(param)\n",
    "    bst.load_model('temp.txt')\n",
    "    preds = bst.predict(dtest)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learningBasedEvaluation(depth, feature_type):\n",
    "    #generateFeatureFile(TRAIN, TEST, corpus_seg, feature_type)\n",
    "    preds = xgboostTraining(depth, 'max')\n",
    "    \n",
    "    point = 0\n",
    "    for i in range(len(TEST)):\n",
    "        if (preds[i] == RELATION[TEST[i][2]]):\n",
    "            point += 1\n",
    "    print(point / len(TEST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergedEvaluation(xgboost_preds, rule_based_result, threshold):\n",
    "    error = 0\n",
    "    preds = xgboost_preds\n",
    "    result = rule_based_result\n",
    "    \n",
    "    for i in range(112):\n",
    "        prob = np.amax(preds[i])\n",
    "        label = preds[i].tolist().index(prob)\n",
    "        test_label = TEST[i][2]\n",
    "        if prob < threshold:\n",
    "            if result[i] != test_label:\n",
    "                error += 1;\n",
    "        else:\n",
    "            if (label != RELATION[test_label]):\n",
    "                error += 1;\n",
    "    return(1 - error/112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person:\n",
    "    def __init__(self, name):\n",
    "        self.name = name;\n",
    "        self.alias = '';\n",
    "        self.weight = 0\n",
    "        self.lastName = name[0];\n",
    "        self.gender = 0 # Unknown: 0, Male: 1, Female: 2\n",
    "        self.hasMaleHead = False\n",
    "        self.hasAliasName = False\n",
    "        self.hasServantFeature = False\n",
    "        self.hasLastName = False\n",
    "        self.initialize()\n",
    "    def initialize(self):\n",
    "        # 冠夫姓\n",
    "        for f in featureDic['夫姓']:\n",
    "            if (f in self.name):\n",
    "                self.hasMaleHead = True\n",
    "                break\n",
    "        # 女性\n",
    "        for f in featureDic['女性']:\n",
    "            if (f in self.name):\n",
    "                self.gender = 2\n",
    "                break\n",
    "                \n",
    "        # 常用名字稱呼\n",
    "        if (len(self.name) == 3 and self.name[1:] not in GENERAL_NAME):\n",
    "            self.alias = self.name[1:]\n",
    "            self.hasAliasName = True\n",
    "            \n",
    "        # 僕人特徵\n",
    "        if (self.name[-1] in ['奴', '兒']):\n",
    "            self.hasServantFeature = True\n",
    "        \n",
    "        #姓\n",
    "        if (self.name[0] in ['賈', '薛', '史', '林', '王', '尤', '李', '傅', '裘', '劉']):\n",
    "                self.hasLastName = True\n",
    "    def compareLastName(self, p2):\n",
    "        if self.lastName == p2.lastName:\n",
    "            return True\n",
    "        return False\n",
    "    def validName(self):\n",
    "        if (self.hasAliasName):\n",
    "            return self.alias\n",
    "        return self.name\n",
    "    def isFemale(self):\n",
    "        if (self.gender == 2):\n",
    "            return True\n",
    "        return False\n",
    "    def isServant(self):\n",
    "        if (self.hasServantFeature):\n",
    "            return True\n",
    "        return False\n",
    "    def isImportant(self):\n",
    "        if (len(self.name) == 3 or self.hasLastName):\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ruleBase(testData, weight_lastName):\n",
    "\n",
    "    preFile = open('Segpreprocess.txt','w') \n",
    "\n",
    "    rightRelation=[]\n",
    "    judgeRelation=[]\n",
    "    \n",
    "    # search corpus by two entities and relationship\n",
    "    for row in testData:\n",
    "        oneLine=[]\n",
    "        oneSentence=[]\n",
    "        threeSentences=[]\n",
    "        oneParagraph=[]\n",
    "        rightRelation.append(row[2])\n",
    "        \n",
    "        # Rule: 其中一個是地方就不用做feature list判斷了，一定是居處\n",
    "        if (row[0] in featureDic['地點']) or (row[1] in featureDic['地點']):\n",
    "            judgeRelation.append('居處')        \n",
    "            continue\n",
    "            \n",
    "        per1 = Person(row[0])\n",
    "        per2 = Person(row[1])\n",
    "        \n",
    "        # Rule: 兩人之中有一人為僕人\n",
    "        if per1.isServant() != per2.isServant():\n",
    "            judgeRelation.append('主僕')\n",
    "            continue\n",
    "        \n",
    "        per1.weight = term_weight_dic[per1.name] if per1.name in term_weight_dic else 0\n",
    "        per2.weight = term_weight_dic[per2.name] if per2.name in term_weight_dic else 0\n",
    "        \n",
    "        \n",
    "        # Rule: 三個字人名如果有姓，去掉比較好找。若最後一個字是「娘」代表是姨娘，姓不可以省略\n",
    "        entity1 = per1.validName()\n",
    "        entity2 = per2.validName()\n",
    "\n",
    "            \n",
    "        preFile.write(row[0]+' '+row[1]+' '+row[2]+'\\n')\n",
    "        for paragraph in corpus:\n",
    "            if ((entity1 in paragraph) and (entity2 in paragraph)):\n",
    "                inLine = False\n",
    "                inSentence = False\n",
    "                inThreeSentences = False\n",
    "                lines = re.split('[，；。？！]', paragraph)\n",
    "                for line in lines:\n",
    "                    if ((entity1 in line) and (entity2 in line)):\n",
    "                        inLine = True\n",
    "                        oneLine.append(line)\n",
    "                        preFile.write(\"LINE:\"+line+'\\n')\n",
    "\n",
    "                sentences = re.split('[。？！]', paragraph)\n",
    "                thrSentences = []\n",
    "\n",
    "                for sentence in sentences:\n",
    "                    idx=sentences.index(sentence)\n",
    "                    # create 3-sentences group\n",
    "                    if idx>1:\n",
    "                        thrSentences.append(sentences[idx-2]+\"。\"+sentences[idx-1]+\"。\"+sentences[idx])#中間標點統一以。代替\n",
    "                    # judge if in the list\n",
    "                    if ((entity1 in sentence) and (entity2 in sentence)):\n",
    "                        inSentence = True\n",
    "                        # 不能跟前面重複，中間沒逗點再加\n",
    "                        commaLoc=[m.start() for m in re.finditer('[，；]', sentence)] # get all locations of '，；' \n",
    "                        hasCommaBetween = False\n",
    "                        for cLoc in commaLoc:#暫不考慮同一人名一句話出現兩次的特例\n",
    "                            a = sentence.find(row[0])\n",
    "                            b = sentence.find(row[1])\n",
    "                            if ((a<cLoc and cLoc<b) or (b<cLoc and cLoc<a)):\n",
    "                                hasCommaBetween = True\n",
    "                                break\n",
    "                        if (hasCommaBetween == True):\n",
    "                            oneSentence.append(sentence)\n",
    "                            preFile.write(\"SENTENCE:\"+sentence+'\\n')\n",
    "\n",
    "                for context in thrSentences:\n",
    "                    if ((entity1 in context) and (entity2 in context)):\n",
    "                        inThreeSentences = True\n",
    "                        # 不能跟前面重複，中間沒。再加\n",
    "                        periodLoc=[m.start() for m in re.finditer('[。]', context)] # get all locations of '。' \n",
    "\n",
    "                        hasPeriodBetween = False\n",
    "                        for pLoc in periodLoc:#暫不考慮同一人名一句話出現兩次的特例\n",
    "                            a = context.find(entity1)\n",
    "                            b = context.find(entity2)\n",
    "                            if ((a<pLoc and pLoc<b) or (b<pLoc and pLoc<a)):\n",
    "                                hasPeriodBetween = True\n",
    "                                break\n",
    "                        if (hasPeriodBetween == True):\n",
    "                            threeSentences.append(context)\n",
    "                            preFile.write(\"CONTEXT:\"+context+'\\n')\n",
    "\n",
    "\n",
    "                if not (inLine or inSentence or inThreeSentences):\n",
    "                    oneParagraph.append(paragraph)\n",
    "                    preFile.write(\"PARAGRAPH:\"+paragraph+'\\n')\n",
    "\n",
    "        # create a dictionary to store appear phrase weight            \n",
    "        term_weight_vector={}\n",
    "        for line in oneLine:\n",
    "            tempLine = line\n",
    "            for term in term_dic:\n",
    "                if term in tempLine:            \n",
    "                    if term not in term_weight_vector:\n",
    "                        term_weight_vector[term]=0\n",
    "                    term_weight_vector[term] += tempLine.count(term) * 16\n",
    "\n",
    "        for context in threeSentences:\n",
    "            tempContext = context\n",
    "            for term in term_dic:\n",
    "                if term in tempContext:            \n",
    "                    if term not in term_weight_vector:\n",
    "                        term_weight_vector[term]=0\n",
    "                    term_weight_vector[term] += tempContext.count(term) * 4\n",
    "                \n",
    "        for sentence in oneSentence:\n",
    "            tempSentence = sentence\n",
    "            for term in term_dic:\n",
    "                if term in tempSentence:            \n",
    "                    if term not in term_weight_vector:\n",
    "                        term_weight_vector[term]=0\n",
    "                    term_weight_vector[term] += tempSentence.count(term) * 2\n",
    "                \n",
    "        for paragraph in oneParagraph:\n",
    "            tempParagraph = paragraph\n",
    "            for term in term_dic:\n",
    "                if term in tempParagraph:            \n",
    "                    if term not in term_weight_vector:\n",
    "                        term_weight_vector[term]=0\n",
    "                    term_weight_vector[term] += tempParagraph.count(term) * 1\n",
    "                \n",
    "        # create feature list that symbolize different relationship\n",
    "        featureList=[0]*12\n",
    "        for term in term_weight_vector:\n",
    "            if term in featureDic['婚配']:\n",
    "                featureList[relationDic['夫妻']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "            elif term in featureDic['直系']:\n",
    "                featureList[relationDic['父子']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "                featureList[relationDic['父女']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "                featureList[relationDic['母子']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "                featureList[relationDic['母女']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "            elif term in featureDic['尊卑']:\n",
    "                featureList[relationDic['祖孫']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "                featureList[relationDic['主僕']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "                featureList[relationDic['夫妻']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "            elif term in featureDic['旁系']:\n",
    "                featureList[relationDic['兄弟姊妹']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "            elif term in featureDic['手足']:\n",
    "                featureList[relationDic['兄弟姊妹']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "                featureList[relationDic['主僕']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "            elif term in featureDic['主僕']:\n",
    "                featureList[relationDic['主僕']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "            elif term in featureDic['命令']:\n",
    "                featureList[relationDic['夫妻']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "                featureList[relationDic['主僕']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "                featureList[relationDic['父子']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "                featureList[relationDic['父女']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "                featureList[relationDic['母子']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "                featureList[relationDic['母女']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "            elif term in featureDic['遠親']:\n",
    "                featureList[relationDic['遠親']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "            elif term in featureDic['師徒']:\n",
    "                featureList[relationDic['師徒']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "\n",
    "\n",
    "        # 同姓：不會是主僕（僕人通常是暱稱），且更有可能是父子、父女、祖孫、兄弟姊妹、姑叔舅姨甥侄、遠親\n",
    "        if per1.compareLastName(per2):\n",
    "            featureList[relationDic['主僕']] = 0\n",
    "            featureList[relationDic['遠親']] *= weight_lastName\n",
    "            featureList[relationDic['父子']] *= weight_lastName\n",
    "            featureList[relationDic['父女']] *= weight_lastName\n",
    "            featureList[relationDic['祖孫']] *= weight_lastName\n",
    "            featureList[relationDic['兄弟姊妹']] *= weight_lastName\n",
    "            featureList[relationDic['姑叔舅姨甥侄']] *= weight_lastName\n",
    "\n",
    "            # 若冠夫姓或父姓，同姓仍有可能是母子或母女\n",
    "            if not per1.hasMaleHead and not per2.hasMaleHead:\n",
    "                featureList[relationDic['母子']]= 0\n",
    "                featureList[relationDic['母女']]= 0\n",
    "            else:\n",
    "                featureList[relationDic['母子']] *= weight_lastName\n",
    "                featureList[relationDic['母女']] *= weight_lastName\n",
    "\n",
    "\n",
    "        # 有女性就不會是父子、師徒\n",
    "        if per1.isFemale() or per2.isFemale():\n",
    "            featureList[relationDic['父子']] = 0\n",
    "            featureList[relationDic['師徒']] = 0\n",
    "\n",
    "\n",
    "        sorted_featureList = sorted(featureList)\n",
    "\n",
    "        key_list = list(relationDic.keys())\n",
    "        value_list = list(relationDic.values())\n",
    "        \n",
    "        # 無特徵值\n",
    "        if sorted_featureList[-1] == 0:\n",
    "            #print('===')\n",
    "            #print(row)\n",
    "            # 同性\n",
    "            if (per1.compareLastName(per2)):\n",
    "                rel = abs(per1.weight - per2.weight)\n",
    "                #print(rel)\n",
    "                #兩者關係不親\n",
    "                if rel > 1:\n",
    "                    #print(row[2] + ':' + '遠親')\n",
    "                    judgeRelation.append('遠親')\n",
    "                    continue\n",
    "                #兩者關係較親密\n",
    "                else:\n",
    "                    #print(row[2] + ':' + '祖孫')\n",
    "                    judgeRelation.append('祖孫')\n",
    "                    continue\n",
    "            elif (per1.isImportant() != per2.isImportant()):\n",
    "                #print(row[2] + ':' + '主僕')\n",
    "                judgeRelation.append('主僕')\n",
    "                continue\n",
    "        result = ''\n",
    "        # 相同的狀況\n",
    "        if (sorted_featureList[-1] == sorted_featureList[-2]):\n",
    "            res1 = featureList.index(sorted_featureList[-1])\n",
    "            res2 = featureList.index(sorted_featureList[-2], res1+1)\n",
    "            res1 = key_list[value_list.index(res1)]\n",
    "            res2 = key_list[value_list.index(res2)]\n",
    "            pairRel = abs(per1.weight - per2.weight)\n",
    "            strongRel = ['夫妻', '父子', '父女', '母女', '母子']\n",
    "            weakRel = ['遠親', '主僕']\n",
    "            if ((pairRel > 1) and (res1 in weakRel) and (res2 not in weakRel)):\n",
    "                result = res1\n",
    "            elif((pairRel <= 1) and (res1 not in strongRel) and (res2 in strongRel)):\n",
    "                result = res1\n",
    "            else:\n",
    "                result = res1\n",
    "\n",
    "        else:\n",
    "            res = featureList.index(sorted_featureList[-1])\n",
    "            result = key_list[value_list[res]]\n",
    "\n",
    "        judgeRelation.append(result)\n",
    "    preFile.close()            \n",
    "\n",
    "    rightRate=0\n",
    "    for i in range(len(judgeRelation)):\n",
    "        if judgeRelation[i]==rightRelation[i]:\n",
    "            rightRate+=1/len(judgeRelation)\n",
    "        #else: \n",
    "            #print(testData[i])\n",
    "            #print(judgeRelation[i]+\"=\"+rightRelation[i])    \n",
    "    #print (\"rightRate=\"+str(rightRate))\n",
    "    return judgeRelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5267857142857143\n"
     ]
    }
   ],
   "source": [
    "generateFeatureFile(TRAIN, TEST, corpus_seg, 'TERMnGROUP')\n",
    "xgboost_preds = xgboostTraining(2, 'prob')\n",
    "rule_based_result = ruleBase(TEST, 2) # param: Evaluted File & Last Name Weight\n",
    "result = mergedEvaluation(xgboost_preds, rule_based_result, 0.140624)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
